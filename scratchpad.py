# -*- coding: utf-8 -*-
"""“scratchpad”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cn--AYL8gY2qsupFj7TICFj6b2xeNfWM
"""



import os
from torch.autograd import Function
import ctypes

lib=ctypes.cdll.LoadLibrary("libmorton/encode.so")
lib.encode.restype=ctypes.c_uint64
"""##DataLoader"""

import numpy as np
import warnings
import os
from torch.utils.data import Dataset
warnings.filterwarnings('ignore')



def pc_normalize(pc):
    centroid = np.mean(pc, axis=0)
    pc = pc - centroid
    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))
    pc = pc / m
    return pc

def farthest_point_sample(point, npoint):
    """
    Input:
        xyz: pointcloud data, [N, D]
        npoint: number of samples
    Return:
        centroids: sampled pointcloud index, [npoint, D]
    """
    N, D = point.shape
    xyz = point[:,:3]
    centroids = np.zeros((npoint,))
    distance = np.ones((N,)) * 1e10
    farthest = np.random.randint(0, N)
    for i in range(npoint):
        centroids[i] = farthest
        centroid = xyz[farthest, :]
        dist = np.sum((xyz - centroid) ** 2, -1)
        mask = dist < distance
        distance[mask] = dist[mask]
        farthest = np.argmax(distance, -1)
    point = point[centroids.astype(np.int32)]
    return point

class ModelNetDataLoader(Dataset):
    def __init__(self, root,  npoint=1024, split='train', uniform=False, normal_channel=True, cache_size=15000):
        self.root = root
        self.npoints = npoint
        self.uniform = uniform
        self.catfile = os.path.join(self.root, 'modelnet40_shape_names.txt')

        self.cat = [line.rstrip() for line in open(self.catfile)]
        self.classes = dict(zip(self.cat, range(len(self.cat))))
        self.normal_channel = normal_channel

        shape_ids = {}
        shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_train.txt'))]
        shape_ids['test'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_test.txt'))]

        assert (split == 'train' or split == 'test')
        shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]
        # list of (shape_name, shape_txt_file_path) tuple
        self.datapath = [(shape_names[i], os.path.join(self.root, shape_names[i], shape_ids[split][i]) + '.txt') for i
                         in range(len(shape_ids[split]))]
        print('The size of %s data is %d'%(split,len(self.datapath)))

        self.cache_size = cache_size  # how many data points to cache in memory
        self.cache = {}  # from index to (point_set, cls) tuple

    def __len__(self):
        return len(self.datapath)

    def _get_item(self, index):
        if index in self.cache:
            point_set, cls = self.cache[index]
        else:
            fn = self.datapath[index]
            cls = self.classes[self.datapath[index][0]]
            cls = np.array([cls]).astype(np.int32)
            point_set = np.loadtxt(fn[1], delimiter=',').astype(np.float32)
            if self.uniform:
                point_set = farthest_point_sample(point_set, self.npoints)
            else:
                point_set = point_set[0:self.npoints,:]

            point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])

            if not self.normal_channel:
                point_set = point_set[:, 0:3]

            if len(self.cache) < self.cache_size:
                self.cache[index] = (point_set, cls)

        return point_set, cls

    def __getitem__(self, index):
        return self._get_item(index)

"""##Provider"""

import numpy as np

def normalize_data(batch_data):
    """ Normalize the batch data, use coordinates of the block centered at origin,
        Input:
            BxNxC array
        Output:
            BxNxC array
    """
    B, N, C = batch_data.shape
    normal_data = np.zeros((B, N, C))
    for b in range(B):
        pc = batch_data[b]
        centroid = np.mean(pc, axis=0)
        pc = pc - centroid
        m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))
        pc = pc / m
        normal_data[b] = pc
    return normal_data


def shuffle_data(data, labels):
    """ Shuffle data and labels.
        Input:
          data: B,N,... numpy array
          label: B,... numpy array
        Return:
          shuffled data, label and shuffle indices
    """
    idx = np.arange(len(labels))
    np.random.shuffle(idx)
    return data[idx, ...], labels[idx], idx

def shuffle_points(batch_data):
    """ Shuffle orders of points in each point cloud -- changes FPS behavior.
        Use the same shuffling idx for the entire batch.
        Input:
            BxNxC array
        Output:
            BxNxC array
    """
    idx = np.arange(batch_data.shape[1])
    np.random.shuffle(idx)
    return batch_data[:,idx,:]

def rotate_point_cloud(batch_data):
    """ Randomly rotate the point clouds to augument the dataset
        rotation is per shape based along up direction
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, rotated batch of point clouds
    """
    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)
    for k in range(batch_data.shape[0]):
        rotation_angle = np.random.uniform() * 2 * np.pi
        cosval = np.cos(rotation_angle)
        sinval = np.sin(rotation_angle)
        rotation_matrix = np.array([[cosval, 0, sinval],
                                    [0, 1, 0],
                                    [-sinval, 0, cosval]])
        shape_pc = batch_data[k, ...]
        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)
    return rotated_data

def rotate_point_cloud_z(batch_data):
    """ Randomly rotate the point clouds to augument the dataset
        rotation is per shape based along up direction
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, rotated batch of point clouds
    """
    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)
    for k in range(batch_data.shape[0]):
        rotation_angle = np.random.uniform() * 2 * np.pi
        cosval = np.cos(rotation_angle)
        sinval = np.sin(rotation_angle)
        rotation_matrix = np.array([[cosval, sinval, 0],
                                    [-sinval, cosval, 0],
                                    [0, 0, 1]])
        shape_pc = batch_data[k, ...]
        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)
    return rotated_data

def rotate_point_cloud_with_normal(batch_xyz_normal):
    ''' Randomly rotate XYZ, normal point cloud.
        Input:
            batch_xyz_normal: B,N,6, first three channels are XYZ, last 3 all normal
        Output:
            B,N,6, rotated XYZ, normal point cloud
    '''
    for k in range(batch_xyz_normal.shape[0]):
        rotation_angle = np.random.uniform() * 2 * np.pi
        cosval = np.cos(rotation_angle)
        sinval = np.sin(rotation_angle)
        rotation_matrix = np.array([[cosval, 0, sinval],
                                    [0, 1, 0],
                                    [-sinval, 0, cosval]])
        shape_pc = batch_xyz_normal[k,:,0:3]
        shape_normal = batch_xyz_normal[k,:,3:6]
        batch_xyz_normal[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)
        batch_xyz_normal[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), rotation_matrix)
    return batch_xyz_normal

def rotate_perturbation_point_cloud_with_normal(batch_data, angle_sigma=0.06, angle_clip=0.18):
    """ Randomly perturb the point clouds by small rotations
        Input:
          BxNx6 array, original batch of point clouds and point normals
        Return:
          BxNx3 array, rotated batch of point clouds
    """
    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)
    for k in range(batch_data.shape[0]):
        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)
        Rx = np.array([[1,0,0],
                       [0,np.cos(angles[0]),-np.sin(angles[0])],
                       [0,np.sin(angles[0]),np.cos(angles[0])]])
        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],
                       [0,1,0],
                       [-np.sin(angles[1]),0,np.cos(angles[1])]])
        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],
                       [np.sin(angles[2]),np.cos(angles[2]),0],
                       [0,0,1]])
        R = np.dot(Rz, np.dot(Ry,Rx))
        shape_pc = batch_data[k,:,0:3]
        shape_normal = batch_data[k,:,3:6]
        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), R)
        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), R)
    return rotated_data


def rotate_point_cloud_by_angle(batch_data, rotation_angle):
    """ Rotate the point cloud along up direction with certain angle.
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, rotated batch of point clouds
    """
    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)
    for k in range(batch_data.shape[0]):
        #rotation_angle = np.random.uniform() * 2 * np.pi
        cosval = np.cos(rotation_angle)
        sinval = np.sin(rotation_angle)
        rotation_matrix = np.array([[cosval, 0, sinval],
                                    [0, 1, 0],
                                    [-sinval, 0, cosval]])
        shape_pc = batch_data[k,:,0:3]
        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)
    return rotated_data

def rotate_point_cloud_by_angle_with_normal(batch_data, rotation_angle):
    """ Rotate the point cloud along up direction with certain angle.
        Input:
          BxNx6 array, original batch of point clouds with normal
          scalar, angle of rotation
        Return:
          BxNx6 array, rotated batch of point clouds iwth normal
    """
    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)
    for k in range(batch_data.shape[0]):
        #rotation_angle = np.random.uniform() * 2 * np.pi
        cosval = np.cos(rotation_angle)
        sinval = np.sin(rotation_angle)
        rotation_matrix = np.array([[cosval, 0, sinval],
                                    [0, 1, 0],
                                    [-sinval, 0, cosval]])
        shape_pc = batch_data[k,:,0:3]
        shape_normal = batch_data[k,:,3:6]
        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)
        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1,3)), rotation_matrix)
    return rotated_data



def rotate_perturbation_point_cloud(batch_data, angle_sigma=0.06, angle_clip=0.18):
    """ Randomly perturb the point clouds by small rotations
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, rotated batch of point clouds
    """
    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)
    for k in range(batch_data.shape[0]):
        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)
        Rx = np.array([[1,0,0],
                       [0,np.cos(angles[0]),-np.sin(angles[0])],
                       [0,np.sin(angles[0]),np.cos(angles[0])]])
        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],
                       [0,1,0],
                       [-np.sin(angles[1]),0,np.cos(angles[1])]])
        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],
                       [np.sin(angles[2]),np.cos(angles[2]),0],
                       [0,0,1]])
        R = np.dot(Rz, np.dot(Ry,Rx))
        shape_pc = batch_data[k, ...]
        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), R)
    return rotated_data


def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):
    """ Randomly jitter points. jittering is per point.
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, jittered batch of point clouds
    """
    B, N, C = batch_data.shape
    assert(clip > 0)
    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)
    jittered_data += batch_data
    return jittered_data

def shift_point_cloud(batch_data, shift_range=0.1):
    """ Randomly shift point cloud. Shift is per point cloud.
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, shifted batch of point clouds
    """
    B, N, C = batch_data.shape
    shifts = np.random.uniform(-shift_range, shift_range, (B,3))
    for batch_index in range(B):
        batch_data[batch_index,:,:] += shifts[batch_index,:]
    return batch_data


def random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):
    """ Randomly scale the point cloud. Scale is per point cloud.
        Input:
            BxNx3 array, original batch of point clouds
        Return:
            BxNx3 array, scaled batch of point clouds
    """
    B, N, C = batch_data.shape
    scales = np.random.uniform(scale_low, scale_high, B)
    for batch_index in range(B):
        batch_data[batch_index,:,:] *= scales[batch_index]
    return batch_data

def random_point_dropout(batch_pc, max_dropout_ratio=0.875):
    ''' batch_pc: BxNx3 '''
    for b in range(batch_pc.shape[0]):
        dropout_ratio =  np.random.random()*max_dropout_ratio # 0~0.875
        drop_idx = np.where(np.random.random((batch_pc.shape[1]))<=dropout_ratio)[0]
        if len(drop_idx)>0:
            batch_pc[b,drop_idx,:] = batch_pc[b,0,:] # set to the first point
    return batch_pc

"""##Model"""

import torch
import torch.nn as nn
import torch.nn.parallel
import torch.utils.data
from torch.autograd import Variable
import numpy as np
import torch.nn.functional as F

"""### Need Morton sort

### Need T-net
"""

def z_order_encode(inputs):
    shape=list(inputs.shape)
    shape[-1]=1
    code=np.ndarray(shape,dtype=np.uint64)
    for i in range(shape[0]):
        for j in range(shape[1]):
            x,y,z=inputs[i,j].tolist()
            code[i,j]=lib.encode(x,y,z)
    return code.astype(np.float64)

class Z_order_sorting(Function):
    @staticmethod
    def forward(ctx,xyz,normal):
        xyz=xyz.permute(0,2,1)
        normal=normal.permute(0,2,1)
        data=((xyz+2)*4096).cpu().numpy()
        data = data.astype(dtype=np.uint32)
        assert data.shape[-1] == 3
        z_order_code=torch.from_numpy(z_order_encode(data)).cuda()
        _,idx=torch.sort(z_order_code,dim=1)
        batch_idx=torch.arange(xyz.shape[0]).reshape(xyz.shape[0],1,1)
        a,b=xyz[batch_idx,idx].squeeze(2),normal[batch_idx,idx].squeeze(2)
        return a,b

    @staticmethod
    def backward(ctx,grad_out):
        return ()
class HilbertSort3D(object):

    def __init__(self, origin=(0,0,0), radius=1.0, bins=32):
        '''
        '''
        self.origin = np.array(origin)
        self.radius = radius
        self.bins = bins
        order = np.log2(bins)
        if order%1.0 > 0.0: raise ValueError("HilbertSort: Bins should be a power of 2.")
        self.curve = self._hilbert_3d(int(np.log2(bins)))

    def _hilbert_3d(self, order):
        '''
        Method generates 3D hilbert curve of desired order.
        Param:
            order - int ; order of curve
        Returns:
            np.array ; list of (x, y, z) coordinates of curve
        '''

        def gen_3d(order, x, y, z, xi, xj, xk, yi, yj, yk, zi, zj, zk, array):
            if order == 0:
                xx = x + (xi + yi + zi)/3
                yy = y + (xj + yj + zj)/3
                zz = z + (xk + yk + zk)/3
                array.append((xx, yy, zz))
            else:
                gen_3d(order-1, x, y, z, yi/2, yj/2, yk/2,
                           zi/2, zj/2, zk/2, xi/2, xj/2, xk/2, array)
                gen_3d(order-1, x + xi/2, y + xj/2, z + xk/2,
                           zi/2, zj/2, zk/2, xi/2, xj/2, xk/2, yi/2, yj/2, yk/2, array)
                gen_3d(order-1, x + xi/2 + yi/2, y + xj/2 + yj/2,
                            z + xk/2 + yk/2, zi/2, zj/2, zk/2,
                            xi/2, xj/2, xk/2, yi/2, yj/2, yk/2, array)
                gen_3d(order-1, x + xi/2 + yi, y + xj/2+ yj,
                            z + xk/2 + yk, -xi/2, -xj/2, -xk/2, -yi/2,
                            -yj/2, -yk/2, zi/2, zj/2, zk/2, array)
                gen_3d(order-1, x + xi/2 + yi + zi/2, y + xj/2 + yj + zj/2,
                            z + xk/2 + yk +zk/2, -xi/2,
                            -xj/2, -xk/2, -yi/2, -yj/2, -yk/2, zi/2, zj/2, zk/2, array)
                gen_3d(order-1, x + xi/2 + yi + zi, y + xj/2 + yj + zj,
                            z + xk/2 + yk + zk, -zi/2, -zj/2,
                            -zk/2, xi/2, xj/2, xk/2, -yi/2, -yj/2, -yk/2, array)
                gen_3d(order-1, x + xi/2 + yi/2 + zi, y + xj/2 + yj/2 + zj ,
                            z + xk/2 + yk/2 + zk, -zi/2,
                            -zj/2, -zk/2, xi/2, xj/2, xk/2, -yi/2, -yj/2, -yk/2, array)
                gen_3d(order-1, x + xi/2 + zi, y + xj/2 + zj,
                            z + xk/2 + zk, yi/2, yj/2, yk/2, -zi/2, -zj/2,
                            -zk/2, -xi/2, -xj/2, -xk/2, array)

        n = pow(2, order)
        hilbert_curve = []
        gen_3d(order, 0, 0, 0, n, 0, 0, 0, n, 0, 0, 0, n, hilbert_curve)

        return np.array(hilbert_curve).astype('int')

    def sort(self, data):
        '''
        Method bins points according to parameters and sorts by traversing binning
        matrix using hilbert space-filling curve.
        Param:
            data - np.array; list of 3D points; (Nx3)
        Returns:
            sorted_data - np.array; list of sorted 3D points; (Nx3)

        '''
        # Center data around origin
###########        data_ = data - self.origin

        # Bin points
        binned = [[[[] for k in range(self.bins)]
                            for j in range(self.bins)]
                                for i in range(self.bins)]      # 32*32*32
        bin_interval = ((self.radius*2) / self.bins)    #每一个bin的宽度
        offset = int(self.radius/bin_interval)      #偏移多少个bin
        for point in data:
            x = int(point[0]/bin_interval) + offset
            y = int(point[1]/bin_interval) + offset
            z = int(point[2]/bin_interval) + offset
            if (x > self.bins-1) or (x < 0): continue
            if (y > self.bins-1) or (y < 0): continue
            if (z > self.bins-1) or (z < 0): continue
            binned[x][y][z].append(point)

        # Traverse and Assemble
        sorted_data = []
        for temp2 in self.curve:
            x = binned[temp2[0]][temp2[1]][temp2[2]]
            if len(x) > 0: sorted_data.append(np.array(x))
        sorted_data = np.concatenate(sorted_data, axis=0)

        return sorted_data

hilbert_sorter=HilbertSort3D(radius=1.35)
class Hilbert_sorting(Function):
    @staticmethod
    def forward(ctx,xyz,normal):
        B,N,C=xyz.shape
        xyz,normal=xyz.cpu(),normal.cpu()
        fused=np.concatenate((xyz,normal),axis=-1)
        for i in range(B):
            fused[i]=hilbert_sorter.sort(fused[i])
        fused=torch.from_numpy(fused).cuda()
        return fused[...,0:3],fused[...,3:6]

    @staticmethod
    def backward(ctx,grad_out):
        return ()

class get_model(nn.Module):
    def __init__(self, k=40, normal_channel=True):
        super(get_model, self).__init__()
        if normal_channel:
            channel = 6
        else:
            channel = 3
        self.conv1 = nn.Conv1d(channel,128,32,4)  #249
        self.bn1=nn.BatchNorm1d(128)
        self.conv2 = nn.Conv1d(128,256,30,3)    #74
        self.bn2=nn.BatchNorm1d(256)
        self.conv3 = nn.Conv1d(256,512,28,2)    #24
        self.bn3=nn.BatchNorm1d(512)
        self.conv4 = nn.Conv1d(512,1024,24)     #1
        self.bn4=nn.BatchNorm1d(1024)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, k)
        self.dropout = nn.Dropout(p=0.4)
        self.bn5 = nn.BatchNorm1d(512)
        self.bn6 = nn.BatchNorm1d(256)

    def forward(self, x):
      #xyz,points=Z_order_sorting.apply(x[:,0:3,:],x[:,3:6,:])
      #x=torch.cat((xyz,points),2)
      x=x.permute(0,2,1)
      xyz,points=Hilbert_sorting.apply(x[...,0:3],x[...,3:6])
      x=torch.cat((xyz,points),2)
      x=x.permute(0,2,1)
      x = F.relu(self.bn1(self.conv1(x)))
      x = F.relu(self.bn2(self.conv2(x)))
      x = F.relu(self.bn3(self.conv3(x)))
      x = F.relu(self.bn4(self.conv4(x)))
      x = torch.flatten(x,start_dim=1)
      x = F.relu(self.bn5(self.fc1(x)))
      x = F.relu(self.bn6(self.dropout(self.fc2(x))))
      x = self.fc3(x)
      x = F.log_softmax(x, dim=1)
      return x

"""##Loss"""

class get_loss(nn.Module):
    def __init__(self):
        super(get_loss, self).__init__()

    def forward(self, pred, target, ):
        total_loss = F.nll_loss(pred, target)

        return total_loss

"""## Training prepare"""

DATA_PATH = 'data/modelnet40_normal_resampled/'
BATCH_SIZE = 16
EPOCH = 250
TRAIN_DATASET = ModelNetDataLoader(root=DATA_PATH, npoint=1024, split='train',normal_channel=True)
TEST_DATASET = ModelNetDataLoader(root=DATA_PATH, npoint=1024, split='test',normal_channel=True)
trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)
testDataLoader = torch.utils.data.DataLoader(TEST_DATASET,  batch_size=BATCH_SIZE, shuffle=False, num_workers=3)
classifier = get_model(40,normal_channel=True).cuda()
print(classifier)
criterion = get_loss().cuda()
optimizer = torch.optim.Adam(
            classifier.parameters(),
            lr=1e-3,
            betas=(0.9, 0.999),
            eps=1e-08,
            weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)
global_epoch = 0
global_step = 0
best_instance_acc = 0.0
best_class_acc = 0.0
mean_correct = []

"""##Training """
from tqdm import tqdm

def test(model, loader, num_class=40):
    mean_correct = []
    class_acc = np.zeros((num_class,3))
    for j, data in tqdm(enumerate(loader), total=len(loader)):
        points, target = data
        target = target[:, 0]
        points = points.transpose(2, 1)
        points, target = points.cuda(), target.cuda()
        classifier = model.eval()
        pred = classifier(points)
        pred_choice = pred.data.max(1)[1]
        for cat in np.unique(target.cpu()):
            classacc = pred_choice[target==cat].eq(target[target==cat].long().data).cpu().sum()
            class_acc[cat,0]+= classacc.item()/float(points[target==cat].size()[0])
            class_acc[cat,1]+=1
        correct = pred_choice.eq(target.long().data).cpu().sum()
        mean_correct.append(correct.item()/float(points.size()[0]))
    class_acc[:,2] =  class_acc[:,0]/ class_acc[:,1]
    class_acc = np.mean(class_acc[:,2])
    instance_acc = np.mean(mean_correct)
    return instance_acc, class_acc

for epoch in range(0,EPOCH):
    scheduler.step(epoch)
    total_loss= 0
    for batch_id, data in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):
        points, target = data
        points = points.data.numpy()
        points = random_point_dropout(points)
        points[:,:, 0:3] = random_scale_point_cloud(points[:,:, 0:3])
        #points[:,:, 0:3] = shift_point_cloud(points[:,:, 0:3])
        points[:,:,0:6]=rotate_point_cloud_with_normal(points[:,:,0:6])
        points = torch.Tensor(points)
        target = target[:, 0]
        points = points.transpose(2, 1)
        points, target = points.cuda(), target.cuda()
        optimizer.zero_grad()
        classifier = classifier.train()
        pred = classifier(points)
        loss = criterion(pred, target.long())
        pred_choice = pred.data.max(1)[1]
        correct = pred_choice.eq(target.long().data).cpu().sum()
        mean_correct.append(correct.item() / float(points.size()[0]))
        loss.backward()
        optimizer.step()
        total_loss+=float(loss)
        global_step += 1
    train_instance_acc = np.mean(mean_correct)
    print('EPOCH %d  Train Instance Accuracy: %f ,mean_loss: %f ' % (epoch,train_instance_acc,total_loss/batch_id))
    with torch.no_grad():
        instance_acc, class_acc = test(classifier.eval(), testDataLoader)
        if (instance_acc >= best_instance_acc):
            best_instance_acc = instance_acc
            state = {
                'epoch': epoch,
                'instance_acc': instance_acc,
                'class_acc': class_acc,
                'model_state_dict': classifier.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }
            torch.save(state, "./seq_conv_best_model.pth")
        if (class_acc >= best_class_acc):
            best_class_acc = class_acc
        print('Test Instance Accuracy: %f, Class Accuracy: %f'% (instance_acc, class_acc))
        print('Best Instance Accuracy: %f, Class Accuracy: %f'% (best_instance_acc, best_class_acc))
        global_epoch += 1
